# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/RNAfolding.ipynb.

# %% auto 0
__all__ = ['DynamicPos', 'Attn_dynamic', 'SE_Block', 'ResConv2dSimple', 'BppTransformerEncoder', 'BppRNAformer']

# %% ../nbs/RNAfolding.ipynb 4
import torch, math, torch.nn as nn, torch.nn.functional as F
from torch.nn import init
from einops import rearrange
from .core import *
from .blocks import *

# %% ../nbs/RNAfolding.ipynb 7
class DynamicPos(nn.Module):
    """
    the output of this module will be added to the attention matrix of the transformer;
    set hidden dimension and number of heads; forward with sequence length;
    output has a shape (nheads, seq_len, seq_len)  
    
    """
    def __init__(self, linear_dim, nheads, depth=2):
        super().__init__()

        units = [1] + [linear_dim]*depth
        self.mlp = make_model(lin,units).append(nn.Linear(linear_dim,nheads))

    @property
    def device(self):
        return next(self.parameters()).device

    def forward(self, inp):
        b,seq_len,h = inp.shape

        # get the (n x n) matrix of distances
        seq_arange = torch.arange(seq_len, device = self.device)
        indices = seq_arange.unsqueeze(-1) - seq_arange.unsqueeze(0)

        # have all number be positive integer
        indices += indices.max()

        # input to continuous positions MLP
        pos = torch.arange(2*seq_len-1,device=self.device).float().unsqueeze(-1) # unsqueeze a dim for mlp
        pos = self.mlp(pos)

        # get position biases
        bias = pos[indices]
        # move the nheads 6 to the front so that it can be added to the attention matrix
        # bias = bias.transpose(0,2).unsqueeze(0)
        bias = rearrange(bias, 'l1 l2 heads -> heads l2 l1')
        return bias

# %% ../nbs/RNAfolding.ipynb 12
class Attn_dynamic(nn.Module):
    
    def __init__(self,
                 embedding_dim,
                 nheads, # embedding_dim should be divisible by nheads
                 dp=0.1):
        super().__init__()

        self.nheads = nheads
        self.attn_chans = embedding_dim//self.nheads
        assert embedding_dim == self.attn_chans*self.nheads, "n embedding is not divisible by nheads"

        self.scale = math.sqrt(embedding_dim/self.nheads)

        self.dynpos = DynamicPos(embedding_dim//4, self.nheads)

        self.norm = nn.LayerNorm(embedding_dim)
        self.qkv = nn.Linear(embedding_dim, embedding_dim*3)

        self.dropout = nn.Dropout(dp)

        # self.proj = nn.Linear(embedding_dim, embedding_dim)
        self.proj = nn.Sequential(
            nn.LayerNorm(embedding_dim),
            lin(embedding_dim,embedding_dim*4, act = nn.GELU,dp=dp),
            lin(embedding_dim*4, embedding_dim,act=None, dp=dp) )


    def forward(self, embedded_seq, bpp=None,mask=None):
        # embedded_seq has a shape of batch, sequence length, and embedding (hidden) dim
        b, l, h = embedded_seq.shape

        # bpp has a shape of h, l, l
        # mask has a shape of b,l

        x = self.norm(embedded_seq) # layer normalize input at the embedding dim

        x = self.qkv(x)
        x = rearrange(x, 'b l (h attn_chans) -> b h l attn_chans', h=self.nheads)
        q,k,v = torch.chunk(x, 3, dim=-1)
        attn = (q @ k.transpose(-1,-2))/self.scale # b, h, l, l

        position_matrix = self.dynpos(embedded_seq) # h, l, l


        attn = attn + position_matrix

        if bpp is not None:
            attn = attn + bpp

        attn = attn.softmax(dim = -1)

        if mask is not None:
            attn = attn*mask[:,None,None,:]

        x = attn @ v  # b, a, l, head

        x = rearrange(x, ' b h l attn_chans -> b l (h attn_chans)', h=self.nheads)

        # skip connect dropout attn with embedded_seq
        x = self.dropout(x) + embedded_seq

        # skip connect proj before with after
        x = x+ self.proj(x)

        return x

# %% ../nbs/RNAfolding.ipynb 21
class SE_Block(nn.Module):
    "credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4"
    def __init__(self, channel, reduction=1):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)
        self.excitation = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        bs, c, l1, l2 = x.shape
        y = self.squeeze(x).squeeze()
        y = self.excitation(y)[:,:,None,None]
        return x * y.expand_as(x)

# %% ../nbs/RNAfolding.ipynb 26
class ResConv2dSimple(nn.Module):
    def __init__(self, in_c, out_c, kernel_size=7, use_se = False):
        super().__init__()

        if not kernel_size % 2:
            raise ValueError("kernel_size should be an odd number")

        layers = [
            nn.Conv2d(in_c, out_c, kernel_size, padding='same', bias=False),
            nn.BatchNorm2d(out_c)
        ]

        if use_se:
            layers.append(SE_Block(out_c))

        layers.append(nn.GELU())
        self.conv = nn.Sequential(*layers)

        if in_c == out_c:
            self.res = nn.Identity()
        else:
            self.res = nn.Sequential(
                nn.Conv2d(in_c, out_c, kernel_size=1, bias=False),
                nn.BatchNorm2d(out_c))

    def forward(self, x):
        # b e s
        x = self.res(x) + self.conv(x)
        return x

# %% ../nbs/RNAfolding.ipynb 30
class BppTransformerEncoder(nn.Module):
    def __init__(self,
                 ni, # embedding dim
                 nheads,
                 ntransformer=12,
                 nconv=3,
                 ks= 3,
                 use_se = True,
                ):
        super().__init__()

        self.num_heads = nheads

        self.transformers = nn.ModuleList([Attn_dynamic(ni,nheads)]*ntransformer)

        convs = [ResConv2dSimple(in_c=1, out_c=nheads,kernel_size=ks, use_se=use_se)]
        for i in range(nconv-1):
            convs.append(ResConv2dSimple(in_c=nheads, out_c=nheads,kernel_size=ks, use_se=use_se))
        self.convs = nn.ModuleList(convs)


    def forward(self, x, bpp, mask=None):
        # bpp has a shape of bs 1 L L
        for ind, mod in enumerate(self.transformers):
            if ind < len(self.convs):
                conv = self.convs[ind]

                # convert bpp shape to bs heads L L
                bpp = conv(bpp)

            x = mod(x, bpp=bpp,mask=mask)

        return x

# %% ../nbs/RNAfolding.ipynb 40
class BppRNAformer(nn.Module):
    def __init__(self, nembed=192, nheads=6,**kwargs):

        super().__init__()

        self.emb = nn.Embedding(4+3,nembed) # 4 nucleotides + 3 tokens
        self.is_good_embed = nn.Embedding(2, nembed)

        self.transformer = BppTransformerEncoder(ni=nembed, nheads = nheads, **kwargs)

        self.proj = nn.Sequential(nn.Linear(nembed, nembed),
                                      nn.GELU(),
                                      nn.Linear(nembed, 2))


    def forward(self, inp):

        # mask
        mask = inp['mask']
        # remove padding tokens to improve performance and accelerate training
        Lmax = mask.sum(-1).max()
        mask = mask[:,:Lmax]

        bpp = inp['bpp']
        # remove padding tokens
        bpp = bpp[:, :Lmax, :Lmax]

        # As bpp values range from 0 to 1, log transform it
        bpp = torch.log(bpp+1e-5)
        # reshape it so that it can go through the conv layer
        bpp = bpp.unsqueeze(1) # bs 1 L L

        # embed seq
        x = self.emb(inp['seq'][:, :Lmax]) # bs seq_length embedding_dim

        # embed sn filter
        e_is_good = self.is_good_embed(inp['sn_filter']) # bs 1 embedding_dim

        # combine
        x = x + e_is_good

        x = self.transformer(x, bpp, mask=mask)

        x = self.proj(x)

        return x
